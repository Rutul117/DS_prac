What is data science?
Data science is an interdisciplinary field that uses scientific methods, algorithms, processes, and systems to extract insights and knowledge from structured and unstructured data. It combines aspects of statistics, mathematics, computer science, and domain expertise to understand complex phenomena and make data-driven decisions.


What is a confusion matrix?
A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows visualization of the performance of an algorithm, typically in terms of accuracy, precision, recall, and F1 score.


Linear and logistic regression?
Linear regression is a statistical method used to model the relationship between two or more variables by fitting a linear equation to observed data. It is commonly used for prediction and forecasting.
Logistic regression is a statistical method used for binary classification tasks. It models the probability of a binary outcome by fitting a logistic curve to the observed data.


Data analytics life-cycle?
The data analytics life-cycle consists of several stages, including data collection, data preparation (cleaning and wrangling), data exploration, data modeling, data evaluation, and deployment of insights. It is a systematic approach to deriving insights and value from data.


Data wrangling?
Data wrangling, also known as data munging, is the process of cleaning, transforming, and enriching raw data into a format suitable for analysis. It involves tasks such as handling missing values, removing duplicates, and restructuring data.


Techniques for handling missing values?
Techniques for handling missing values include deletion (removing rows or columns with missing values), imputation (replacing missing values with estimated values), and advanced methods such as predictive modeling to fill in missing values.


Outliers?
Outliers are data points that deviate significantly from the rest of the data in a dataset. They can arise due to measurement errors, experimental variability, or genuine anomalies in the data.


Techniques to detect outliers?
Techniques to detect outliers include statistical methods such as Z-score, IQR (Interquartile Range), visualization methods such as box plots and scatter plots, and machine learning algorithms designed to identify anomalies.


Hypothesis testing?
Hypothesis testing is a statistical method used to make inferences about a population parameter based on sample data. It involves formulating a null hypothesis and an alternative hypothesis, selecting a significance level, and using statistical tests to determine whether there is enough evidence to reject the null hypothesis.


Null hypothesis?
The null hypothesis (H0) is a statement that there is no significant difference or relationship between variables. It is often the default assumption in hypothesis testing.


Alternative hypothesis?
The alternative hypothesis (H1) is a statement that there is a significant difference or relationship between variables. It is what the researcher is trying to provide evidence for in hypothesis testing.


Data visualization?
Data visualization is the graphical representation of data and information. It allows for the exploration, analysis, and communication of insights in a visual format, using charts, graphs, maps, and other visual elements.


Tools and techniques for data visualization?
Tools for data visualization include software such as Python libraries (Matplotlib, Seaborn), R packages (ggplot2, plotly), and business intelligence tools (Tableau, Power BI). Techniques include bar charts, line charts, scatter plots, histograms, heatmaps, and more.
Decision tree?
A decision tree is a supervised machine learning algorithm used for classification and regression tasks. It recursively splits the data into subsets based on the most significant attribute at each node, resulting in a tree-like structure where leaves represent class labels or numerical values.
KNN?
K-Nearest Neighbors (KNN) is a simple, non-parametric supervised learning algorithm used for classification and regression tasks. It predicts the class or value of a new data point by finding the majority class or averaging the values of its k nearest neighbors in the feature space.
